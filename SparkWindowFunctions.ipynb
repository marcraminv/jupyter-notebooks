{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window functions with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are SQL Window Functions?\n",
    "Window functions operate on a set of rows and return a single value for each row from the underlying query. The term window describes the set of rows on which the function operates. A window function uses values from the rows in a window to calculate the returned values.\n",
    "\n",
    "\n",
    "## Elements of Window functions\n",
    "\n",
    "* Partitioning\n",
    "* Ordering\n",
    "* Framing\n",
    "\n",
    "## SQL Expression\n",
    "\n",
    "```sql\n",
    " window_function (expression) \n",
    " OVER (\n",
    "   [ PARTITION BY expr_list ]\n",
    "   [ ORDER BY order_list ]\n",
    "   [ frame_clause ] \n",
    " )\n",
    "```\n",
    "where function is one of the functions described, such as AVG(), and expr_list is:\n",
    "```\n",
    "expression | column_name [, expr_list ]\n",
    "```\n",
    "and order_list is:\n",
    "\n",
    "```\n",
    "expression | column_name [ASC | DESC] [ NULLS { FIRST | LAST } ] [, order_list ]\n",
    "```\n",
    "and the optional frame_clause is one of the following frames:\n",
    "```\n",
    "{ RANGE | ROWS } frame_start\n",
    "{ RANGE | ROWS } BETWEEN frame_start AND frame_end  \n",
    "```\n",
    "where frame_start is one of the following choices:\n",
    "```\n",
    "UNBOUNDED PRECEDING  \n",
    "CURRENT ROW  \n",
    "```\n",
    "and frame_end is one of the following choices:\n",
    "```\n",
    "CURRENT ROW  \n",
    "UNBOUNDED FOLLOWING  \n",
    "```\n",
    "\n",
    "## How window functions are classified?\n",
    "* __Ranking__\n",
    "\n",
    "    All of the ranking functions depend on the sort ordering specified by the **ORDER BY** clause of the associated window definition. Rows that are not distinct in the ordering are called ___peers___. The ranking functions are defined so that they give the same answer for any two peer rows.\n",
    "    * Row_number()\n",
    "    * rank()\n",
    "    * dense_rank()\n",
    "    * percent_rank()\n",
    "    * ntile\n",
    "    \n",
    "* __Analytics(aka Value functions)__\n",
    "    * cume_dist\n",
    "    * first_value\n",
    "    * last_value\n",
    "    * lag\n",
    "    * lead\n",
    "    \n",
    "* __aggregation__\n",
    "    * avg()\n",
    "    * sum()\n",
    "    * ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Customer0\", \"2019-04-01\", 60),\n",
    "        (\"Customer1\", \"2019-04-01\", 30), \n",
    "        (\"Customer2\", \"2019-04-01\", 40),\n",
    "        (\"Customer3\", \"2019-04-01\", 40),\n",
    "        (\"Customer4\", \"2019-04-01\", 20),\n",
    "        (\"Customer5\", \"2019-04-02\", 20),\n",
    "        (\"Customer6\", \"2019-04-02\", 10),\n",
    "        (\"Customer7\", \"2019-04-02\", 60)\n",
    "    ], \n",
    "    [\"customer_name\", \"date\", \"amount\"]\n",
    ")\n",
    "df.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROW_NUMBER window function determines the ordinal number of the current row within its partition. The ORDER BY expression in the OVER clause determines the number. Each value is ordered within its partition. Rows with equal values for the ORDER BY expressions receive different row numbers nondeterministically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|customer_name|      date|amount|row|\n",
      "+-------------+----------+------+---+\n",
      "|    Customer0|2019-04-01|    60|  1|\n",
      "|    Customer2|2019-04-01|    40|  2|\n",
      "|    Customer3|2019-04-01|    40|  3|\n",
      "|    Customer1|2019-04-01|    30|  4|\n",
      "|    Customer4|2019-04-01|    20|  5|\n",
      "|    Customer7|2019-04-02|    60|  1|\n",
      "|    Customer5|2019-04-02|    20|  2|\n",
      "|    Customer6|2019-04-02|    10|  3|\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, desc\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window function to partition data by CustomerName\n",
    "windowF = Window.partitionBy(\"date\").orderBy(desc(\"amount\"))\n",
    "\n",
    "#Apply window functions to Df\n",
    "result_row_number = df.withColumn(\"row\", row_number().over(windowF))\n",
    "\n",
    "#Result\n",
    "result_row_number.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|customer_name|      date|amount|row|\n",
      "+-------------+----------+------+---+\n",
      "|    Customer0|2019-04-01|    60|  1|\n",
      "|    Customer2|2019-04-01|    40|  2|\n",
      "|    Customer3|2019-04-01|    40|  3|\n",
      "|    Customer1|2019-04-01|    30|  4|\n",
      "|    Customer4|2019-04-01|    20|  5|\n",
      "|    Customer7|2019-04-02|    60|  1|\n",
      "|    Customer5|2019-04-02|    20|  2|\n",
      "|    Customer6|2019-04-02|    10|  3|\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With SQL solution\n",
    "sql_result = spark.sql(\"SELECT \\\n",
    "customer_name, \\\n",
    "date, \\\n",
    "amount, \\\n",
    "row_number() over (partition by date order by amount DESC) as `row` \\\n",
    "from customers\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RANK window function determines the rank of a value in a group of values. The ORDER BY expression in the OVER clause determines the value. Each value is ranked within its partition. Rows with equal values for the ranking criteria receive the same rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|customer_name|      date|amount|row|\n",
      "+-------------+----------+------+---+\n",
      "|    Customer0|2019-04-01|    60|  1|\n",
      "|    Customer2|2019-04-01|    40|  2|\n",
      "|    Customer3|2019-04-01|    40|  2|\n",
      "|    Customer1|2019-04-01|    30|  4|\n",
      "|    Customer4|2019-04-01|    20|  5|\n",
      "|    Customer7|2019-04-02|    60|  1|\n",
      "|    Customer5|2019-04-02|    20|  2|\n",
      "|    Customer6|2019-04-02|    10|  3|\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank, desc\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window function to partition\n",
    "windowF = Window.partitionBy(\"date\").orderBy(desc(\"amount\"))\n",
    "#Apply window functions to Df\n",
    "result_rank = df.withColumn(\"row\", rank().over(windowF))\n",
    "#Result\n",
    "result_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|customer_name|      date|amount|row|\n",
      "+-------------+----------+------+---+\n",
      "|    Customer0|2019-04-01|    60|  1|\n",
      "|    Customer2|2019-04-01|    40|  2|\n",
      "|    Customer3|2019-04-01|    40|  2|\n",
      "|    Customer1|2019-04-01|    30|  4|\n",
      "|    Customer4|2019-04-01|    20|  5|\n",
      "|    Customer7|2019-04-02|    60|  1|\n",
      "|    Customer5|2019-04-02|    20|  2|\n",
      "|    Customer6|2019-04-02|    10|  3|\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With SQL solution\n",
    "sql_result = spark.sql(\"SELECT \\\n",
    "customer_name, \\\n",
    "date, \\\n",
    "amount, \\\n",
    "rank() over (partition by date order by amount DESC) as `row` \\\n",
    "from customers\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DENSE_RANK () window function determines the rank of a value in a group of values based on the ORDER BY expression and the OVER clause. Each value is ranked within its partition. Rows with equal values receive the same rank. There are no gaps in the sequence of ranked values if two or more rows have the same rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|customer_name|      date|amount|row|\n",
      "+-------------+----------+------+---+\n",
      "|    Customer0|2019-04-01|    60|  1|\n",
      "|    Customer2|2019-04-01|    40|  2|\n",
      "|    Customer3|2019-04-01|    40|  2|\n",
      "|    Customer1|2019-04-01|    30|  3|\n",
      "|    Customer4|2019-04-01|    20|  4|\n",
      "|    Customer7|2019-04-02|    60|  1|\n",
      "|    Customer5|2019-04-02|    20|  2|\n",
      "|    Customer6|2019-04-02|    10|  3|\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank, desc\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window function to partition\n",
    "windowF = Window.partitionBy(\"date\").orderBy(desc(\"amount\"))\n",
    "#Apply window functions to Df\n",
    "result_dense_rank = df.withColumn(\"row\", dense_rank().over(windowF))\n",
    "#Result\n",
    "result_dense_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|customer_name|      date|amount|row|\n",
      "+-------------+----------+------+---+\n",
      "|    Customer0|2019-04-01|    60|  1|\n",
      "|    Customer2|2019-04-01|    40|  2|\n",
      "|    Customer3|2019-04-01|    40|  2|\n",
      "|    Customer1|2019-04-01|    30|  3|\n",
      "|    Customer4|2019-04-01|    20|  4|\n",
      "|    Customer7|2019-04-02|    60|  1|\n",
      "|    Customer5|2019-04-02|    20|  2|\n",
      "|    Customer6|2019-04-02|    10|  3|\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With SQL solution\n",
    "sql_result = spark.sql(\"SELECT \\\n",
    "customer_name, \\\n",
    "date, \\\n",
    "amount, \\\n",
    "dense_rank() over (partition by date order by amount DESC) as `row` \\\n",
    "from customers\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## percent_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PERCENT_RANK () window function calculates the percent rank of the current row using the following formula: (x - 1) / (number of rows in window partition - 1) where x is the rank of the current row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|customer_name|      date|amount| row|\n",
      "+-------------+----------+------+----+\n",
      "|    Customer0|2019-04-01|    60| 0.0|\n",
      "|    Customer2|2019-04-01|    40|0.25|\n",
      "|    Customer3|2019-04-01|    40|0.25|\n",
      "|    Customer1|2019-04-01|    30|0.75|\n",
      "|    Customer4|2019-04-01|    20| 1.0|\n",
      "|    Customer7|2019-04-02|    60| 0.0|\n",
      "|    Customer5|2019-04-02|    20| 0.5|\n",
      "|    Customer6|2019-04-02|    10| 1.0|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank, desc\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window function to partition\n",
    "windowF = Window.partitionBy(\"date\").orderBy(desc(\"amount\"))\n",
    "#Apply window functions to Df\n",
    "result_percent_rank = df.withColumn(\"row\", percent_rank().over(windowF))\n",
    "#Result\n",
    "result_percent_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|customer_name|      date|amount| row|\n",
      "+-------------+----------+------+----+\n",
      "|    Customer0|2019-04-01|    60| 0.0|\n",
      "|    Customer2|2019-04-01|    40|0.25|\n",
      "|    Customer3|2019-04-01|    40|0.25|\n",
      "|    Customer1|2019-04-01|    30|0.75|\n",
      "|    Customer4|2019-04-01|    20| 1.0|\n",
      "|    Customer7|2019-04-02|    60| 0.0|\n",
      "|    Customer5|2019-04-02|    20| 0.5|\n",
      "|    Customer6|2019-04-02|    10| 1.0|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With SQL solution\n",
    "sql_result = spark.sql(\"SELECT \\\n",
    "customer_name, \\\n",
    "date, \\\n",
    "amount, \\\n",
    "percent_rank() over (partition by date order by amount DESC) as `row` \\\n",
    "from customers\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Frames\n",
    "\n",
    "In addition to the ordering and partitioning, users need to define:\n",
    "\n",
    "- Start boundary of the frame\n",
    "- End boundary of the frame, \n",
    "- Type of the frame\n",
    "\n",
    "There are five types of boundaries:\n",
    "\n",
    "- UNBOUNDED PRECEDING\n",
    "- UNBOUNDED FOLLOWING\n",
    "- CURRENT ROW \n",
    "- *value* PRECEDING \n",
    "- *value* FOLLOWING\n",
    "\n",
    "UNBOUNDED PRECEDING and UNBOUNDED FOLLOWING represent the first row of the partition and the last row of the partition, respectively.  \n",
    "\n",
    "For the other three types of boundaries, they specify the offset from the position of the current input row and their specific meanings are defined based on the type of the frame. \n",
    "    \n",
    "There are two types of frames:\n",
    "\n",
    "- ROW frame \n",
    "- RANGE frame\n",
    "    \n",
    "to define a window specification, users can use the following syntax in SQL.\n",
    "```sql\n",
    "OVER (PARTITION BY ... ORDER BY ... [frame_type] BETWEEN start AND end)\n",
    "\n",
    "```\n",
    "Here, frame_type can be either ROWS (for ROW frame) or RANGE (for RANGE frame); start can be any of UNBOUNDED PRECEDING, CURRENT ROW, <value> PRECEDING, and <value> FOLLOWING; and end can be any of UNBOUNDED FOLLOWING, CURRENT ROW, <value> PRECEDING, and <value> FOLLOWING.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LAG() window function returns the value for the row before the current row in a partition. If no row exists, null is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|customer_name|      date|amount| lag|\n",
      "+-------------+----------+------+----+\n",
      "|    Customer0|2019-04-01|    60|null|\n",
      "|    Customer2|2019-04-01|    40|  60|\n",
      "|    Customer3|2019-04-01|    40|  40|\n",
      "|    Customer1|2019-04-01|    30|  40|\n",
      "|    Customer4|2019-04-01|    20|  30|\n",
      "|    Customer7|2019-04-02|    60|null|\n",
      "|    Customer5|2019-04-02|    20|  60|\n",
      "|    Customer6|2019-04-02|    10|  20|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, desc\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window function to partition\n",
    "windowF = Window.partitionBy(\"date\").orderBy(desc(\"amount\"))\n",
    "#Apply window functions to Df\n",
    "result_lag = df.withColumn(\"lag\", lag('amount',1).over(windowF))\n",
    "#Result\n",
    "result_lag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|customer_name|      date|amount| row|\n",
      "+-------------+----------+------+----+\n",
      "|    Customer0|2019-04-01|    60|null|\n",
      "|    Customer2|2019-04-01|    40|  60|\n",
      "|    Customer3|2019-04-01|    40|  40|\n",
      "|    Customer1|2019-04-01|    30|  40|\n",
      "|    Customer4|2019-04-01|    20|  30|\n",
      "|    Customer7|2019-04-02|    60|null|\n",
      "|    Customer5|2019-04-02|    20|  60|\n",
      "|    Customer6|2019-04-02|    10|  20|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With SQL solution\n",
    "sql_result = spark.sql(\"SELECT \\\n",
    "customer_name, \\\n",
    "date, \\\n",
    "amount, \\\n",
    "lag(amount, 1) over (partition by date order by amount DESC) as `row` \\\n",
    "from customers\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LEAD() window function returns the value for the row after the current row in a partition. If no row exists, null is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|customer_name|      date|amount|lead|\n",
      "+-------------+----------+------+----+\n",
      "|    Customer0|2019-04-01|    60|  40|\n",
      "|    Customer2|2019-04-01|    40|  40|\n",
      "|    Customer3|2019-04-01|    40|  30|\n",
      "|    Customer1|2019-04-01|    30|  20|\n",
      "|    Customer4|2019-04-01|    20|null|\n",
      "|    Customer7|2019-04-02|    60|  20|\n",
      "|    Customer5|2019-04-02|    20|  10|\n",
      "|    Customer6|2019-04-02|    10|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, desc\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window function to partition\n",
    "windowF = Window.partitionBy(\"date\").orderBy(desc(\"amount\"))\n",
    "#Apply window functions to Df\n",
    "result_lead = df.withColumn(\"lead\", lead('amount',1).over(windowF))\n",
    "#Result\n",
    "result_lead.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|customer_name|      date|amount| row|\n",
      "+-------------+----------+------+----+\n",
      "|    Customer0|2019-04-01|    60|  40|\n",
      "|    Customer2|2019-04-01|    40|  40|\n",
      "|    Customer3|2019-04-01|    40|  30|\n",
      "|    Customer1|2019-04-01|    30|  20|\n",
      "|    Customer4|2019-04-01|    20|null|\n",
      "|    Customer7|2019-04-02|    60|  20|\n",
      "|    Customer5|2019-04-02|    20|  10|\n",
      "|    Customer6|2019-04-02|    10|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With SQL solution\n",
    "sql_result = spark.sql(\"SELECT \\\n",
    "customer_name, \\\n",
    "date, \\\n",
    "amount, \\\n",
    "lead(amount,1) over (partition by date order by amount DESC) as `row` \\\n",
    "from customers\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LAST_VALUE window function returns the value of the specified expression with respect to the last row in the window frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|customer_name|      date|amount|last|\n",
      "+-------------+----------+------+----+\n",
      "|    Customer0|2019-04-01|    60|  20|\n",
      "|    Customer2|2019-04-01|    40|  20|\n",
      "|    Customer3|2019-04-01|    40|  20|\n",
      "|    Customer1|2019-04-01|    30|  20|\n",
      "|    Customer4|2019-04-01|    20|  20|\n",
      "|    Customer7|2019-04-02|    60|  10|\n",
      "|    Customer5|2019-04-02|    20|  10|\n",
      "|    Customer6|2019-04-02|    10|  10|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import last,avg, desc\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Window function to partition\n",
    "# You need to add explicitly the frame type to run correctly\n",
    "windowF = Window \\\n",
    ".partitionBy(\"date\") \\\n",
    ".orderBy(desc(\"amount\")) \\\n",
    ".rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "#Apply window functions to Df\n",
    "result_last = df.withColumn(\"last\", last('amount').over(windowF))\n",
    "#Result\n",
    "result_last.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|customer_name|      date|amount|row|\n",
      "+-------------+----------+------+---+\n",
      "|    Customer0|2019-04-01|    60| 20|\n",
      "|    Customer2|2019-04-01|    40| 20|\n",
      "|    Customer3|2019-04-01|    40| 20|\n",
      "|    Customer1|2019-04-01|    30| 20|\n",
      "|    Customer4|2019-04-01|    20| 20|\n",
      "|    Customer7|2019-04-02|    60| 10|\n",
      "|    Customer5|2019-04-02|    20| 10|\n",
      "|    Customer6|2019-04-02|    10| 10|\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With SQL solution\n",
    "sql_result = spark.sql(\"SELECT \\\n",
    "customer_name, \\\n",
    "date, \\\n",
    "amount, \\\n",
    "last(amount) \\\n",
    "over (\\\n",
    "partition by date \\\n",
    "order by amount desc \\\n",
    "rows between UNBOUNDED PRECEDING and UNBOUNDED FOLLOWING) as `row` \\\n",
    "from customers\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc reference:\n",
    "- [Introduction to window function Spark SQL by Databricks](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)\n",
    "- [Apache Drill window function introduction](https://drill.apache.org/docs/sql-window-functions-introduction/)\n",
    "- [Mastering Spark SQL by @jaceklaskowski](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
